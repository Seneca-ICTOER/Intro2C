name: algolia-scraper
on:
  push: # only run on push events
    branches:
      - main
  check_suite: # only run check suite events
    types: [completed]
  workflow_dispatch: # only run on workflow dispatch events
jobs:
  scrape:
    runs-on: ubuntu-latest # specifies the OS image to use for the job
    steps: # specifies the commands to run
      - name: check out code ðŸ›Ž
        uses: actions/checkout@v2
      - name: scrape the site ðŸ§½
        env: #- specifies the environment variables
          APPLICATION_ID: ${{ secrets.APPLICATION_ID }} #generated when you create a project on algolia website after you sign up
          WRITE_API_KEY: ${{ secrets.API_KEY }} #API key be sure to have the right permissions
          SITE_URL: ${{ secrets.SITE_URL }} #site url
          INDEX_NAME: ${{ secrets.INDEX_NAME }} #docusaurus or anything if you want ot change it
          # run: create .env file and run bash script scrape.sh
        run: |
          touch .env  
          ./scrape.sh

  # 1 - you would need to sign up on www.aligolia.com and create a project for the purpose of creating index
  # 2 - add an index_name called "docusaurus" for instance
  # 3 - you add API_key and give rights for ACLs search, addObject, deleteObject, editSearch
  # 4 - site_url is the deployed site, but siteindex needs to match to crawl it
  # 5 -if u look at the pull request ff0a9a5 i have a sample .env file with the variables set.

